{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Top Down Table Based Parsing\n",
    "#### Authors: Can Birol, Ryan Lambert, Sandip Sidhu \n",
    "\n",
    "This document will discuss the methods for parsing code and more generally grammars in a top down table based method. So what is top down table based parsing? To explain there are two key parts here, top down and table based. Top down parsing is analysing an input in a left to right way generating a leftmost derivation by expanding the given formal grammar rules. Table based refers to how the data structure is formed, specifically in this case a stack and transition table is used. This is in contrast to recursive descent parsers which have implicit stacks created by the recursive structure. \n",
    "\n",
    "For this document the focus will be on LL(k) parsers, more specifically LL(1) parsing. The reason for this because this document is intended to be an introductory manual for how to do top down parsing and LL(1) grammars are relatively easy to construct and understand. Table based parsing also has the advantage of being able to show the intermediate steps that occur from the input to the output. This can show users and help them understand how a language can be parsed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "Below are some terms that will be used throughout the notebook that we thought it would be helpful to briefly define before going into detail.\n",
    "\n",
    "**Parse:** To parse, or parsing, in English, is analyzing a sentence, dividing it into it's parts and identifying each of it's parts and their relation to each other. In the context of computer science, parsing refers to analyzing strings of symbols and checking them against the rules of a grammar.\n",
    "\n",
    "**Bottom-up parsing:** Refers to how the parser recognizes lowest level details first and works it's way up to the highest level details. \n",
    "\n",
    "**Top-down parsing:** Inversely to bottom-up, top-down parsers recognize the highest level details first and works it's way down to the lowest level details.\n",
    "\n",
    "**Grammar:** A grammar is simply the set of production rules that a string must follow.\n",
    "\n",
    "**Context-free grammar:** Is a grammar that includes all possible strings in a given language.\n",
    "\n",
    "**Lookahead:** Refers to how far in advance the parser can see in order to determine which rule to use. In the context of parsers it is usually represented by the k value.\n",
    "\n",
    "**Backtracking:** Refers to the parsers ability to reverse and look at another option. For example, you're in a maze and reach a dead end, so you reverse and take the other available path.\"\n",
    "\n",
    "**FIRST and FOLLOW:** FIRST refers to the set of terminals that can appear in the first position of any string. Whereas FOLLOW is the union between a FIRST and any non-terminal that follows it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## History of Top Down Table Parsers\n",
    "\n",
    "LR parsing was invented by Donald Knuth in 1965. Knuth proved that LR(k) grammars can be parsed with an execution time essentially proportional to the length of the program, and that every LR(k) grammar for k > 1 can be mechanically transformed into an LR(1) grammar for the same language. LR, standing for **L**eft to right with **R**ightmost derivation, was named as such because of the way it builds the parse tree, reading from Left to right, and then parsing bottom up with a rightmost derivation. The k value is the \"lookahead\" value which references the parsers ability to see input symbols ahead of the current one in order to decide how it is to be parsed. The k value is usually 1.\n",
    "\n",
    "In 1969, Frank DeRemer invented the first subset of LR parsers, the LALR Parser or Look-Ahead LR Parser in his PhD paper \"Practical Translators for LR(k) languages\". In the paper it showed that if you have a language that is recognized by both an LR(0) parser and an LALR parser, the LALR parser will require the same amount of states as the LR(0) parser however the LALR parser will have more language recognition power. It was also found that the LALR parser was more memory efficient which makes it powerful enough for modern programming languages like Java. The LALR parser also has a k value. Like the LA parser, the k value in LALR(k) also represents the \"lookahead\". There is also a common LALR parser with a k value of two, meaning a two-token lookahead (LALR(2)). Both the LR parsers and LALR parsers can have a indefinite k-token lookup value, however these are rare as they are hard to implement. Later on in 1979 Frank DeRemer and Tom Pennello significantly improved the memory efficiency with a series of optimizations.\n",
    "\n",
    "However both the LR parser and the LALR parser are bottom up parsers. Though the innovation of the LR parser paved the way for top down parsers to come. Introducing the LL parser.\n",
    "\n",
    "The idea of LL(1) grammars was introduced by Lewis and Stearns in 1968 in their paper \"Syntax directed transduction\". LL parsers also use the k \"lookahead\" value like the LR and LALR parsers do. LL parsers parse from left to right, however, they parse from top to bottom instead of vise versa. Thus the name \"top down parser\". LL parsers also do this without backtracking at all which makes them very valuable as they are much easier to construct than their bottom up counterparts. Though the one downside is that LL parsers cannot recognize all context free languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## First example\n",
    "\n",
    "In this example we will be parsing a very simple grammar. This grammar is essentially the character a, b, and c and the ability for them to be added within parenthesis. Full production rules below: \n",
    "1. S -> F\n",
    "2. S -> (S + F)\n",
    "3. F -> a\n",
    "4. F -> b\n",
    "5. F -> c\n",
    "\n",
    "First we will generate for ourselves a parse table based on this grammar with the terminals as columns and non terminals as rows:\n",
    "\n",
    "|      | ( | ) | a | b | c | + | $ |invalid|\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "|   S  |   1  |   -1 |   0  |  0   |  0   |  -1  |   -1 |   -1 |\n",
    "|  F   |  -1  | -1   |  2   |   3  |   4  |  -1  |  -1  | -1   |\n",
    "\n",
    "Now we can code this example as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#All constants are indexed from 0\n",
    "sleeptime = 2\n",
    "\n",
    "Term = 0\n",
    "Rule = 1\n",
    "\n",
    "# Terminals\n",
    "T_LPAR = 0\n",
    "T_RPAR = 1\n",
    "T_A = 2\n",
    "T_B = 3\n",
    "T_C = 4\n",
    "T_PLUS = 5\n",
    "T_END = 6\n",
    "T_INVALID = 7\n",
    "\n",
    "# Non-terminals\n",
    "N_S = 0\n",
    "N_F = 1\n",
    "\n",
    "#parse table\n",
    "table = [[ 1, -1, 0, 0, 0, -1, -1, -1],\n",
    "         [-1, -1, 2, 3, 4, -1, -1, -1]]\n",
    "\n",
    "rules = [[(Rule,N_F)],\n",
    "         [(Term,T_LPAR), (Rule,N_S), (Term,T_PLUS), (Rule,N_F), (Term,T_RPAR)],\n",
    "         [(Term,T_A)], \n",
    "         [(Term,T_B)],\n",
    "         [(Term,T_C)]]\n",
    "\n",
    "stack = [(Term,T_END), (Rule,N_S)]\n",
    "\n",
    "def tokenize(inputstring):\n",
    "    print('Tokenize')\n",
    "    tokens = []\n",
    "    for c in inputstring:\n",
    "        if c   == '+': tokens.append(T_PLUS)\n",
    "        elif c == '(': tokens.append(T_LPAR)\n",
    "        elif c == ')': tokens.append(T_RPAR)\n",
    "        elif c == 'a': tokens.append(T_A)\n",
    "        elif c == 'b': tokens.append(T_B)\n",
    "        elif c == 'c': tokens.append(T_C)\n",
    "        else: tokens.append(T_INVALID)\n",
    "    tokens.append(T_END)\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def TopDownTableParse(tokens):\n",
    "    print('Parse')\n",
    "    position = 0\n",
    "    while len(stack) > 0:\n",
    "        (stype, svalue) = stack.pop()\n",
    "        token = tokens[position]\n",
    "        if stype == Term:        \n",
    "            if svalue == token:\n",
    "                position += 1\n",
    "                print('Pop', svalue)\n",
    "                if token == T_END:\n",
    "                    print('Input accepted')\n",
    "                    break\n",
    "            else:\n",
    "                print('Bad term on input:', token)\n",
    "                break\n",
    "        elif stype == Rule:\n",
    "            print('Current svalue', svalue, 'and token', token)\n",
    "            rule = table[svalue][token]\n",
    "            print('Applying rule', rule)\n",
    "            for r in reversed(rules[rule]):\n",
    "                stack.append(r)\n",
    "        print('Current stack', stack)\n",
    "        time.sleep(sleeptime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize\n",
      "[0, 2, 5, 4, 1, 6]\n",
      "Parse\n",
      "Current svalue 0 and token 0\n",
      "Applying rule 1\n",
      "Current stack [(0, 6), (0, 1), (1, 1), (0, 5), (1, 0), (0, 0)]\n",
      "Pop 0\n",
      "Current stack [(0, 6), (0, 1), (1, 1), (0, 5), (1, 0)]\n",
      "Current svalue 0 and token 2\n",
      "Applying rule 0\n",
      "Current stack [(0, 6), (0, 1), (1, 1), (0, 5), (1, 1)]\n",
      "Current svalue 1 and token 2\n",
      "Applying rule 2\n",
      "Current stack [(0, 6), (0, 1), (1, 1), (0, 5), (0, 2)]\n",
      "Pop 2\n",
      "Current stack [(0, 6), (0, 1), (1, 1), (0, 5)]\n",
      "Pop 5\n",
      "Current stack [(0, 6), (0, 1), (1, 1)]\n",
      "Current svalue 1 and token 4\n",
      "Applying rule 4\n",
      "Current stack [(0, 6), (0, 1), (0, 4)]\n",
      "Pop 4\n",
      "Current stack [(0, 6), (0, 1)]\n",
      "Pop 1\n",
      "Current stack [(0, 6)]\n",
      "Pop 6\n",
      "Input accepted\n"
     ]
    }
   ],
   "source": [
    "inputstring = '(a+c)'\n",
    "TopDownTableParse(tokenize(inputstring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Output analysis\n",
    "The parser performs three types of steps depending on whether the top of the stack is a nonterminal, a terminal or the special symbol \\$:\n",
    "\n",
    "- If the top is a nonterminal (this section is represented in the elif stype == Rule branch) then it looks up in the parsing table on the basis of this nonterminal (represented by svalue) and the symbol on the input stream (represented by token) which rule of the grammar it should use to replace it with on the stack. If the parsing table indicates that there is no such rule then it reports an error and stops.\n",
    "- If the top is a terminal (represented by the if stype == Term branch) then it compares it to the symbol on the input stream and if they are equal they are both removed. If they are not equal the parser reports an error and stops.\n",
    "- If the top is \\$ and on the input stream there is also a \\$ (this is represented by the sub-branch if token == T_END) then the parser reports that it has successfully parsed the input, otherwise it reports an error. In both cases the parser will stop.\n",
    "\n",
    "These steps are repeated until the parser stops, and then it will have either completely parsed the input or it will have reported an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Formal Definition of an LL(1) Grammar\n",
    "In the most basic sense, grammars that are context free and whose parsing table has no multiple entries in it are considered LL(1). LL(1) grammars are non ambiguous and also non left-recursive.\n",
    "\n",
    "For example, if we have a grammar that is context free defined as **G = (V<sub>T</sub>,V<sub>N</sub>,S,P)**, and for every nonterminal and every string **a**, **b** where **a** =/= **b** and **A** -> **a** | **b**, two rules must also be true in order for the grammar to be LL(1):\n",
    "\n",
    "**Rule 1:** FIRST(**a**) **intersect** FIRST(**b**) = **empty set** \n",
    "\n",
    "**Rule 2:** if **a** => **epsilon** then FIRST(**b**) **intersect** FOLLOW(**A**) = **empty set**\n",
    "\n",
    "If these two rules remain true for your grammar, then it is LL(1).\n",
    "\n",
    "*note: jupyter would not allow the insertion of greek and set theory characters. Therefore they were replaced with english characters and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Steps to create LL1 parsing table\n",
    "\n",
    "#### Step 1 Remove alternations:\n",
    "\n",
    "A_1 = X_1 X_2 X_3...X_n_1a | X_1 X_2 X_3...X_n_1b    \n",
    "\n",
    "                                                   \n",
    "A_2 = X_1 X_2 X_3...X_n_2  \n",
    "\n",
    "\n",
    "A_3 = X_1 X_2 X_3...X_n_3a  | X_1 X_2 X_3...X_n_3b  \n",
    "           \n",
    "           . . .\n",
    "           \n",
    "A_k = X_1 X_2 X_3...X_n_k\n",
    "\n",
    "#### The altered grammar looks like this:\n",
    "\n",
    "\n",
    "A_1 = X_1 X_2 X_3...X_n_1a\n",
    "\n",
    "A_1 = X_1 X_2 X_3...X_n_1b \n",
    "\n",
    "A_2 = X_1 X_2 X_3...X_n_2  \n",
    "\n",
    "A_3 = X_1 X_2 X_3...X_n_3a \n",
    "\n",
    "A_3 = X_1 X_2 X_3...X_n_3b\n",
    "\n",
    "           . . .\n",
    "           \n",
    "A_k = X_1 X_2 X_3...X_n_k\n",
    "\n",
    "### Step 2 create FIRST sets for the right-hand sides of each of the grammarâs productions:\n",
    "\n",
    "A_k = X_1 X_2 X_3...X_n_k\n",
    "###### FIRST(X) for arbitrary x, we start by defining FIRST(X), for a single symbol X (a terminal, a nonterminal, or Î±):\n",
    "##### Put FIRST(X1) â {Îµ} into FIRST(X). \n",
    "##### If Îµ is in FIRST(X1), then put F IRST(X2) â {Îµ} into F IRST(X). â If Îµ is in F IRST(X2), then put F IRST(X3) â {Îµ} into FIRST(X). â . . . â If Îµ is in FIRST(Xi) for 1 \u0016 i \u0016 k (all production right- hand sides) then put Îµ into F IRST(X).\n",
    "\n",
    "FIRST(A_1)=\n",
    "\n",
    "FIRST(A_2)=\n",
    "\n",
    "FIRST(A_3)=\n",
    "\n",
    "    . . . \n",
    "\n",
    "FIRST(A_k)=\n",
    "\n",
    "### Step 3 create Follow sets for the right-hand sides of each of the grammarâs productions:\n",
    "\n",
    "A_k = X_1 X_2 X_3...X_n_k\n",
    "\n",
    "#### For each production X â Î±AÎ², put FIRST(Î²) â {Ç«} in FOLLOW(A)\n",
    "If Ç« is in FIRST(Î²) then put FOLLOW(X) into FOLLOW(A)\n",
    "For each production X â Î±A, put FOLLOW(X) into FOLLOW(A)\n",
    "\n",
    "FOLLOW(A_1)=\n",
    "\n",
    "FOLLOW(A_2)=\n",
    "\n",
    "FOLLOW(A_3)=\n",
    "\n",
    "     . . .\n",
    "\n",
    "FOLLOW(A_k)=\n",
    "\n",
    "### Step 4 create Follow sets for the right-hand sides of each of the grammarâs productions:\n",
    "\n",
    "for each production X â Î±\n",
    "\n",
    "â for each terminal t in First(Î±): \n",
    "        put Î± in Table[X,t]\n",
    "\n",
    "â if Î± is in First(Î±) then:\n",
    "       for each terminal t in Follow(X): put Î± in Table[X,t]\n",
    "\n",
    "\n",
    "|      | X_1 | X_2 | . | . | . | . |X_n_k|\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "|  A_1  |   P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n",
    "|  A_2   |   P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n",
    "|  A_3   |    P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n",
    "|  .   |    P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n",
    "|  .   |    P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n",
    "|  .  |    P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n",
    "|  A_K  |    P_1  |  P_2 |  .  |.   | .   |  .  |   P_k |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Steps to create Parsing Table \n",
    "A -> BC\n",
    "\n",
    "C -> +BC | Îµ\n",
    "\n",
    "B -> DF\n",
    "\n",
    "F -> *DF | Îµ\n",
    "\n",
    "D -> (E) | id\n",
    "\n",
    "Step 1: Take grammar and remove alterations\n",
    "\n",
    "A -> BC\n",
    "\n",
    "C -> +BC\n",
    "\n",
    "C -> Îµ\n",
    "\n",
    "B -> DF\n",
    "\n",
    "F -> *DF\n",
    "\n",
    "F -> Îµ\n",
    "\n",
    "D -> (A)\n",
    "\n",
    "D -> id\n",
    "\n",
    "Step 2:\n",
    "\n",
    "FIRST(A)={(,id}\n",
    "\n",
    "FIRST(C)={+,Îµ}\n",
    "\n",
    "FIRST(B)={(,id}\n",
    "\n",
    "FIRST(F)={*,Îµ}\n",
    "\n",
    "FIRST(D)={(,id}\n",
    "\n",
    "step 3:\n",
    "\n",
    "FOLLOW(A)={$,)}\n",
    "\n",
    "FOLLOW(C)={$,)}\n",
    "\n",
    "FOLLOW(B)={+,$,)}\n",
    "\n",
    "FOLLOW(F)={+,$,)}\n",
    "\n",
    "FOLLOW(D)={*,+,$,)}\n",
    "\n",
    "step 4:\n",
    "\n",
    "|      | id | + | * | ( | ) | \\$ |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "|  A  | A -> BC  |  | A -> BC |  |  |    |\n",
    "| C  |  | C -> +BC|    |   | F -> Îµ   |    |   \n",
    "|  B   |B -> DF |  |   |B -> DF   |    |    | \n",
    "| F |    |  F -> Îµ|  B -> DF  |  |  F -> Îµ | F -> Îµ  |  \n",
    "|  D  |    D -> id  |   |    |D -> (A)   |  |    | |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Limitations of top down table parsers\n",
    "\n",
    "In order for a CFG to be an LL(1) grammar, certain conflicts must not arise, which we describe in this section.\n",
    "\n",
    "Let A be a non-terminal. Then FIRST(A) is the set of terminals that can appear in the first position of any string derived from A. FOLLOW(A) is the union over FIRST(B) where B is any non-terminal that immediately follows A in the right hand side of a production rule.\n",
    "\n",
    "There are two main types of conflicts First/First and First/Follow. These conflicts occurs as a result of ambiguity in the rule to choose. There needs to be a single choice per entry in the table. If there are more, the program does not know what path to take, as a result it is not parsable by LL(1):\n",
    "\n",
    "#### FIRST/FIRST\n",
    "The FIRST sets of two different grammar rules for the same non-terminal intersect. That is that the first position of a string derived from A is also the first position of a string derived from B. \n",
    "\n",
    "Example of First/First error producing grammar:\n",
    "1. S -> E | E 'a'\n",
    "2. E -> 'b' | Îµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print('S -> E ')\n",
    "print('S -> E ''\\'a\\'')\n",
    "print('E -> ''\\'b\\'')\n",
    "print('E -> Îµ')\n",
    "time.sleep(1)\n",
    "print(\"FIRST(S)=FIRST(E)\")\n",
    "time.sleep(1)\n",
    "print(\"FIRST(E)={'b', Îµ}\")\n",
    "time.sleep(1)\n",
    "print(\"FIRST(S)={E 'a'}\")\n",
    "time.sleep(1)\n",
    "print(\"FIRST(E 'a')={'b',a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import *\n",
    "# here we define e to be Îµ\n",
    "def ani():\n",
    "    counter=1\n",
    "    rule1='S -> E '\n",
    "    rule2='S -> E ''\\'a\\''\n",
    "    rule3='E -> ''\\'b\\''\n",
    "    rule4='E -> e'\n",
    "    for i in range(3):\n",
    "        clear_output()\n",
    "        if i==0:\n",
    "            \n",
    "            data=[\n",
    "                 [' ','a','b','e'],\n",
    "                 ['E','' ,'',''],\n",
    "                 ['S','',''  ,''],\n",
    "                 ]\n",
    "    \n",
    "\n",
    "        elif i==1:\n",
    "            data = [\n",
    "                 [' ','a','b','e'],\n",
    "                 ['E',rule4 ,rule3,rule4],\n",
    "                 ['S','','','']\n",
    "                 ]\n",
    "\n",
    "              \n",
    "        elif i==2:\n",
    "            \n",
    "            data = [ [' ','a','b','e'],['E',rule4 ,rule3,rule4],['S',rule2,rule1 +' or '+ rule2 ,rule1]]\n",
    "        display(HTML('<table><tr>{}</tr></table>'.format('</tr><tr>'.join('<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data))))\n",
    "        time.sleep(2.5)\n",
    "ani()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### FIRST/FOLLOW\n",
    "The FIRST and FOLLOW set of a grammar rule overlap. That is an element in the FIRST(A) exists in FOLLOW(A) creating a conflict.\n",
    "\n",
    "First/Follow Error Example Grammar:\n",
    "1. S-> A 'a' 'b'\n",
    "2. A-> 'a' | Îµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('FIRST(S)=A ')\n",
    "print('FOLLOW(S)=$')\n",
    "time.sleep(1)\n",
    "print('FIRST(A)={a,Îµ}')\n",
    "print('FOLLOW(A)={a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import *\n",
    "# here we define e to be Îµ\n",
    "def ani():\n",
    "\n",
    "    rule1='S-> A \\'a\\' \\'b\\''\n",
    "    rule2='A -> \\'a\\''\n",
    "    rule3='A -> e'\n",
    "    for i in range(2):\n",
    "        clear_output()\n",
    "        if i==0:\n",
    "            \n",
    "            data=[\n",
    "                 [' ','a','b','e'],\n",
    "                 ['A','' ,'',''],\n",
    "                 ['S','',''  ,''],\n",
    "                 ]\n",
    "    \n",
    "\n",
    "        elif i==1:\n",
    "            data = [\n",
    "                 [' ','a','b','e'],\n",
    "                 ['E',rule1+\" or \"+rule2 ,\"\",''],\n",
    "                 ['S','','','']\n",
    "                 ]\n",
    "\n",
    "        display(HTML('<table><tr>{}</tr></table>'.format('</tr><tr>'.join('<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data))))\n",
    "        time.sleep(2.5)\n",
    "ani()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## More Complex Example\n",
    "\n",
    "In this example we will construct a LL(1) parser for the following grammar:\n",
    "1. S -> S + T | S - T | S + C | S - C | T | C\n",
    "2. T -> a | b | c\n",
    "3. C -> 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n",
    "\n",
    "This grammar essentially allows for addition and subtraction of single digit constants or up to three variables called a, b, and c. This grammar has an initial First/Follow and First/First conflict due to the left recursion so we need to remove that first. We so an example of this below: \n",
    "1. S -> T E | C E\n",
    "2. E -> + T E | - T E | + C E| - C E | e \n",
    "3. T -> a | b | c\n",
    "4. C -> 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n",
    "\n",
    "However this is not good enough as the second rule still has a First/First conflict so we will alter the grammar again as shown below:\n",
    "1. S -> F E\n",
    "2. E -> + F E \n",
    "3. E -> - F E\n",
    "4. F -> T \n",
    "5. F -> C\n",
    "6. T -> a\n",
    "7. T -> b\n",
    "8. T -> c\n",
    "9. C -> 0\n",
    "10. C -> 1\n",
    "11. C -> 2\n",
    "12. C -> 3\n",
    "13. C -> 4\n",
    "14. C -> 5\n",
    "15. C -> 6\n",
    "16. C -> 7\n",
    "17. C -> 8\n",
    "18. C -> 9\n",
    "19. E -> Empty\n",
    "\n",
    "\n",
    "Now this grammar is conflict free so we can construct a table for it:\n",
    "\n",
    "|      | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | a    | b    | c    | +    | -    | \\$   |invalid|\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "|  S   | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | -1   | -1   | -1   | -1   |\n",
    "|  E   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | 2    | 3    | 19   | -1   |\n",
    "|  F   | 5    | 5    | 5    | 5    | 5    | 5    | 5    | 5    | 5    | 5    | 4    | 4    | 4    | -1   | -1   | -1   | -1   |\n",
    "|  T   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | -1   | 6    | 7    | 8    | -1   | -1   | -1   | -1   |\n",
    "|  C   | 9    | 10   | 11   | 12   | 13   | 14   | 15   | 16   | 17   | 18   | -1   | -1   | -1   | -1   | -1   | -1   | -1   |\n",
    "\n",
    "And now we will construct the parser as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#All constants are indexed from 0\n",
    "sleeptime = 2\n",
    "\n",
    "Term = 0\n",
    "Rule = 1\n",
    "\n",
    "# Terminals\n",
    "#The constants 0-9 are given the corresponding value, this is implicit in the way that the program works\n",
    "T_A = 10\n",
    "T_B = 11\n",
    "T_C = 12\n",
    "T_PLUS = 13\n",
    "T_MINUS = 14\n",
    "T_END = 15\n",
    "T_INVALID = 16\n",
    "\n",
    "# Non-terminals\n",
    "N_S = 0\n",
    "N_E = 1\n",
    "N_F = 2\n",
    "N_T = 3\n",
    "N_C = 4\n",
    "\n",
    "#parse table\n",
    "table = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1],\n",
    "         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 2, 18, -1],\n",
    "         [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, -1, -1, -1, -1],\n",
    "         [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, 6, 7, -1, -1, -1, -1],\n",
    "         [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, -1, -1, -1, -1, -1, -1, -1]]\n",
    "\n",
    "rules = [[(Rule,N_F), (Rule, N_E)],\n",
    "         [(Term,T_PLUS), (Rule,N_F), (Rule,N_E)],\n",
    "         [(Term,T_MINUS), (Rule,N_F), (Rule,N_E)],\n",
    "         [(Rule, N_T)],\n",
    "         [(Rule, N_C)],\n",
    "         [(Term,T_A)], \n",
    "         [(Term,T_B)],\n",
    "         [(Term,T_C)], \n",
    "         [(Term,0)], \n",
    "         [(Term,1)], \n",
    "         [(Term,2)], \n",
    "         [(Term,3)], \n",
    "         [(Term,4)], \n",
    "         [(Term,5)], \n",
    "         [(Term,6)], \n",
    "         [(Term,7)], \n",
    "         [(Term,8)], \n",
    "         [(Term,9)],\n",
    "         []]\n",
    "\n",
    "stack = [(Term,T_END), (Rule,N_S)]\n",
    "\n",
    "def tokenizeComplex(inputstring):\n",
    "    print('Tokenize')\n",
    "    tokens = []\n",
    "    for c in inputstring:\n",
    "        if c   == '+': tokens.append(T_PLUS)\n",
    "        elif c == '-': tokens.append(T_MINUS)\n",
    "        elif c == 'a': tokens.append(T_A)\n",
    "        elif c == 'b': tokens.append(T_B)\n",
    "        elif c == 'c': tokens.append(T_C)\n",
    "        elif c == '0': tokens.append(0)\n",
    "        elif c == '1': tokens.append(1)\n",
    "        elif c == '2': tokens.append(2)\n",
    "        elif c == '3': tokens.append(3)\n",
    "        elif c == '4': tokens.append(4)\n",
    "        elif c == '5': tokens.append(5)\n",
    "        elif c == '6': tokens.append(6)\n",
    "        elif c == '7': tokens.append(7)\n",
    "        elif c == '8': tokens.append(8)\n",
    "        elif c == '9': tokens.append(9)\n",
    "        else: tokens.append(T_INVALID)\n",
    "    tokens.append(T_END)\n",
    "    print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def topDownTableParseComplex(tokens):\n",
    "    print('Parse')\n",
    "    position = 0\n",
    "    print('Current stack', stack)\n",
    "    while len(stack) > 0:\n",
    "        (stype, svalue) = stack.pop()\n",
    "        token = tokens[position]\n",
    "        if stype == Term:        \n",
    "            if svalue == token:\n",
    "                position += 1\n",
    "                print('Pop', svalue)\n",
    "                if token == T_END:\n",
    "                    print('Input accepted')\n",
    "                    break\n",
    "            else:\n",
    "                print('Bad term on input:', token)\n",
    "                break\n",
    "        elif stype == Rule:\n",
    "            print('Current svalue', svalue, 'and token', token)\n",
    "            rule = table[svalue][token]\n",
    "            print('Applying rule', rule)\n",
    "            for r in reversed(rules[rule]):\n",
    "                stack.append(r)\n",
    "        print('Current stack', stack)\n",
    "        time.sleep(sleeptime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputstring = '4+a-c'\n",
    "topDownTableParseComplex(tokenizeComplex(inputstring)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LL(1) Parser Parser\n",
    "\n",
    "In this section a parser for grammars in BNF form will be described, then implemented in python below. First a definition of symbols used in our BNF to be read. All non-terminal symbols must be enclosed in <>, terminals must be enclosed in single quotations ' ', ::= is the production symbol, | is used for alteration and ; for EoL. There are two reserved characters \\$ and e, to represent EoF and epsilon respectively (changing which character is reserved for epsilon is possible by changing the epsilon variable at the top of the program). Ex. the first example in our BNF would look like:\n",
    "0. < S > ::= < F >;\n",
    "1. < S > ::= '(' < S > '+' < F > ')';\n",
    "2. < F > ::= 'a';\n",
    "3. < F > ::= 'b';\n",
    "4. < F > ::= 'c';\n",
    "\n",
    "note: there is very limited error handling so grammars that are improperly formatted will create errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "T_ = []\n",
    "N_ = []\n",
    "epsilon = 'e'\n",
    "first = [[]]\n",
    "rules = [[]]\n",
    "follow = [[]]\n",
    "\n",
    "def grammarParser(inputString):\n",
    "    global T_\n",
    "    global N_\n",
    "    global first\n",
    "    global rules\n",
    "    global follow\n",
    "    global epsilon\n",
    "    printTable = [[]]\n",
    "    table = [[]]\n",
    "    \n",
    "    inputString = ''.join(inputString.split()) #this strips the inputString of whitespace\n",
    "    \n",
    "    if initNandT(inputString) == -1:\n",
    "        return\n",
    "    T_.append(\"$\")\n",
    "    T_.append(\"Invalid\")\n",
    "    \n",
    "    #initialize the printTable for human output and the table which will store the result\n",
    "    w, h = len(T_), len(N_)\n",
    "    printTable = [[-1] * (w+1) for n in range(h+1)]\n",
    "    printTable[0][0] = \" \"\n",
    "    table = [[-1] * (w) for m in range(h)]\n",
    "    i = 1\n",
    "    for t in T_:\n",
    "        printTable[0][i] = t\n",
    "        i += 1\n",
    "    i = 1\n",
    "    \n",
    "    for n in N_:\n",
    "        printTable[i][0] = n\n",
    "        i+=1\n",
    "    \n",
    "    print(\"Initial Human Readable table: \")\n",
    "    for s in printTable:\n",
    "        print(s)\n",
    "    \n",
    "    #initialize the rules list\n",
    "    ruleStrings = inputString[:len(inputString)-1].split(\";\") #this is hardcoded this way because a grammar should end with a ;\n",
    "    \n",
    "    w, h = 2, 0\n",
    "    for s in ruleStrings:\n",
    "        h += s.count(\"|\")\n",
    "    h += len(ruleStrings)\n",
    "    rules = [[\"\"] * (w) for i in range(h)]\n",
    "    #this section reformats alterations into new rules\n",
    "    index = 0\n",
    "    for s in ruleStrings:\n",
    "        abi = s.find(\">\") #angle bracket index\n",
    "        ruleChar = s[1:abi]\n",
    "        s = s[s.find(\"=\")+1:]  \n",
    "        barIndex = s.find(\"|\")\n",
    "        if barIndex != -1: #begin processing alterations\n",
    "            tempS = s.split(\"|\")\n",
    "            for x in tempS:\n",
    "                rules[index][0] = ruleChar\n",
    "                production = helper(x)\n",
    "                rules[index][1] = production\n",
    "                index+=1\n",
    "        else: #this handles adding rules with no alterations\n",
    "            rules[index][0] = ruleChar\n",
    "            production = helper(s)\n",
    "            rules[index][1] = production\n",
    "            index +=1\n",
    "    print(\"Rules array: \\n\", rules)\n",
    "    \n",
    "    #now we will compute the FIRST and FOLLOW sets\n",
    "    w, h = 2, len(N_)\n",
    "    first = [[\"\"] * (w) for i in range(h)]\n",
    "    follow = [[\"\"] * (w) for i in range(h)]\n",
    "    i = 0\n",
    "    for n in N_:\n",
    "        first[i][0] = n\n",
    "        first[i][1] = []\n",
    "        follow[i][0] = n\n",
    "        follow[i][1] = []\n",
    "        i+=1\n",
    "        \n",
    "    #compute first\n",
    "    changing = 1 \n",
    "    failsafe = 0\n",
    "    while changing and (failsafe<10):\n",
    "        changing = 0\n",
    "        for rule in rules:\n",
    "            index = N_.index(rule[0]) #this stores the index of the nonterminal lhs of the rule\n",
    "            temp = first[index][1]\n",
    "            first[index][1] = union(first[index][1], computefirst(rule[1]))\n",
    "            if changingcheck(first[index][1], temp):\n",
    "                changing = 1\n",
    "        failsafe += 1\n",
    "    \n",
    "    print(\"The FIRST array: \\n\", first)    \n",
    "    #compute follow\n",
    "    follow[0][1] = [\"$\"] #start non terminal has EoF in its follow\n",
    "    changing = 1\n",
    "    failsafe = 0\n",
    "    while changing and (failsafe<10):\n",
    "        changing = 0\n",
    "        for rule in rules:\n",
    "            for x in rule[1]:\n",
    "                if x in N_:   # the body of this loop is over all the nonterms on the rhs\n",
    "                    nonterm = N_.index(x)\n",
    "                    index = rule[1].index(x)\n",
    "                    if index+1 < len(rule[1]):\n",
    "                        comFir = computefirst(rule[1][index+1:])\n",
    "                        temp = follow[nonterm][1]\n",
    "                        follow[nonterm][1] = union(follow[nonterm][1], excludeEpsilon(comFir))\n",
    "                        if epsilon in comFir:\n",
    "                             follow[nonterm][1] = union(follow[nonterm][1], follow[N_.index(rule[0])][1])\n",
    "                        if changingcheck(follow[nonterm][1], temp):\n",
    "                            changing = 1\n",
    "                    elif \"$\" not in follow[nonterm][1]:\n",
    "                            follow[nonterm][1].append(\"$\")\n",
    "                            changing = 1\n",
    "        failsafe += 1\n",
    "    print(\"The FOLLOW array: \\n\", follow)\n",
    "    \n",
    "    i = 0 #i here will represent the rule's index value\n",
    "    for rule in rules:\n",
    "        lhs = rule[0]\n",
    "        nonterm = N_.index(rule[0])\n",
    "        rhs = rule[1]\n",
    "        tmp = computefirst(rhs)\n",
    "        if epsilon in tmp:\n",
    "            tmp = union(tmp, follow[nonterm][1])\n",
    "            tmp.remove(epsilon)\n",
    "        for t in tmp:\n",
    "            term = T_.index(t)\n",
    "            if table[nonterm][term] != -1:\n",
    "                print(\"Grammar is not LL(1)\"); return -1\n",
    "            table[nonterm][term] = i\n",
    "            printTable[nonterm+1][term+1] = i\n",
    "        i += 1\n",
    "    \n",
    "    print(\"Final Human Readable table: \")\n",
    "    for s in printTable:\n",
    "        print(s)\n",
    "        \n",
    "    return table\n",
    "def computefirst(rhs):\n",
    "    global first\n",
    "    global epsilon\n",
    "    tmp = []\n",
    "    i = 0\n",
    "    etest = True\n",
    "    while i < len(rhs):\n",
    "        if rhs[i] in T_:\n",
    "            tmp.append(rhs[i])\n",
    "            etest = False\n",
    "            break\n",
    "        else:\n",
    "            index = N_.index(rhs[i]) #grab the index of the nonterminal found\n",
    "            tmp = union(tmp, excludeEpsilon(first[index][1]))\n",
    "            if epsilon not in first[index][1]:\n",
    "                etest = False\n",
    "                break\n",
    "        i+=1\n",
    "    if etest:\n",
    "        tmp.extend(epsilon)\n",
    "    return tmp\n",
    "def excludeEpsilon(arr):\n",
    "    global epsilon\n",
    "    if epsilon in arr:\n",
    "        arr.remove(epsilon)\n",
    "    return arr\n",
    "        \n",
    "def changingcheck(a, b):\n",
    "    for x in a:\n",
    "        if x not in b:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def union(a, b):\n",
    "    return list(set(a) | set(b))\n",
    "\n",
    "def initNandT(inputString):\n",
    "    global T_\n",
    "    global N_\n",
    "    T_ = []\n",
    "    N_ = []\n",
    "    currentID = \"\"\n",
    "    i = 0;\n",
    "    while i < len(inputString): \n",
    "        c = inputString[i]\n",
    "        if c == '<':\n",
    "            while 1:\n",
    "                i +=1\n",
    "                if (i < len(inputString)):\n",
    "                    c = inputString[i]\n",
    "                    if (c == '>'): \n",
    "                        break\n",
    "                    else:\n",
    "                        currentID += c\n",
    "                else: \n",
    "                    print(\"reached EoF before closing >\"); return -1\n",
    "                \n",
    "            if currentID not in N_:\n",
    "                N_.append(currentID)\n",
    "            currentID = \"\"\n",
    "        elif c == '\\'':\n",
    "            while 1:\n",
    "                i +=1\n",
    "                if (i < len(inputString)):\n",
    "                    c = inputString[i]\n",
    "                    if (c == '\\''): \n",
    "                        break\n",
    "                    else:\n",
    "                        currentID += c\n",
    "                else: \n",
    "                    print(\"reached EoF before closing \\'\"); return -1\n",
    "                \n",
    "            if currentID not in T_:\n",
    "                T_.append(currentID)\n",
    "            currentID = \"\"\n",
    "        i+=1\n",
    "    return 0\n",
    "def helper(s):\n",
    "    \n",
    "    i = 0\n",
    "    returnl = []\n",
    "    while i < len(s):\n",
    "        temp3 = \"\"\n",
    "        if s[i] == \"<\":\n",
    "            while 1:\n",
    "                i +=1\n",
    "                if (s[i] == '>'): break #this is guaranteed now so extra checks are not needed\n",
    "                temp3 = temp3 + s[i]\n",
    "        elif s[i] == '\\'':\n",
    "            while 1:\n",
    "                i +=1\n",
    "                if (s[i] == '\\''): break #this is guaranteed now so extra checks are not needed\n",
    "                temp3 = temp3 + s[i]\n",
    "        returnl.extend(temp3)\n",
    "        i+=1\n",
    "    return returnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Human Readable table: \n",
      "[' ', '(', '+', ')', 'a', 'b', 'c', '$', 'Invalid']\n",
      "['S', -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "['F', -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "Rules array: \n",
      " [['S', ['F']], ['S', ['(', 'S', '+', 'F', ')']], ['F', ['a']], ['F', ['b']], ['F', ['c']]]\n",
      "The FIRST array: \n",
      " [['S', ['a', 'c', 'b', '(']], ['F', ['a', 'c', 'b']]]\n",
      "The FOLLOW array: \n",
      " [['S', ['+', '$']], ['F', [')', '$']]]\n",
      "Final Human Readable table: \n",
      "[' ', '(', '+', ')', 'a', 'b', 'c', '$', 'Invalid']\n",
      "['S', 1, -1, -1, 0, 0, 0, -1, -1]\n",
      "['F', -1, -1, -1, 2, 3, 4, -1, -1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, -1, -1, 0, 0, 0, -1, -1], [-1, -1, -1, 2, 3, 4, -1, -1]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammarParser(\"< S > ::= < F >; < S > ::= '(' < S > '+' < F > ')';< F > ::= 'a' | 'b' | 'c';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Human Readable table: \n",
      "[' ', 'e', 'a', 'b', '$', 'Invalid']\n",
      "['S', -1, -1, -1, -1, -1]\n",
      "['A', -1, -1, -1, -1, -1]\n",
      "['B', -1, -1, -1, -1, -1]\n",
      "Rules array: \n",
      " [['S', ['A']], ['S', ['B']], ['A', ['e']], ['A', ['a']], ['B', ['A', 'b']], ['B', ['b']]]\n",
      "The FIRST array: \n",
      " [['S', ['a', 'b']], ['A', ['a']], ['B', ['a', 'b']]]\n",
      "The FOLLOW array: \n",
      " [['S', ['$']], ['A', ['b', '$']], ['B', ['$']]]\n",
      "Grammar is not LL(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is an example of an ambigous grammar that looks like it should be LL(1) but isn't\n",
    "grammarParser(\"<S>::= <A> | <B>; <A>::= 'e' | 'a';<B>::=<A>'b'|'b';\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why table parsing has been replaced and what we can learn from it\n",
    "\n",
    "This section will discuss the exact differences between LL(1) parsers and LALR(1) parsers. Specifically will discuss YACC and the ease of creating LALR(1) parsers and the difficulties with LL(1) regarding context. \n",
    "\n",
    "It is possible that LL(1) parsers and LALR(1) parsers are the same. If all symbols with empty derivations in the LL(1) grammar have non-empty derivations, the LL(1) and LALR(1) parsers are considered equal. The difficulty with generating LL(1) parsers as opposed to their LALR(1) counterparts is that there are some context free languages that are not recognized by the LL(1) that are by the LALR(1). This coupled with the fact that LALR parsers are more memory efficient gives them the edge. With the difficulty that top-down parsing has on analysis of semantics and the stark limitations of LL(k), LALR parsers have proven to be a little more versatile and easier to construct in nature.\n",
    "\n",
    "YACC (stands for Yet Another Compiler Compiler) is a unix program that generates LALR (**L**ook **A**head **L**eft-to-Right **R**ight-derivation) parsers. It was created in the early 70's by Stephen C. Johnson. YACC became so popular that for a time it was the default parser generator on most UNIX based computers and became the basis for many parser generators to come in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Biblography \n",
    "\n",
    "Aho, Alfred V., \"Compilers : principles, techniques, & tools\", Pearson/Addison Wesley, 2007.\n",
    "\n",
    "DeRemer, Frank; Pennello, Thomas; \"Efficient Computation of LALR(1) Look-Ahead Sets\", October 1982\n",
    "\n",
    "Dick Grune, Cerial Jacobs, \"Parsing Techniques\", Springer, 2008.\n",
    "\n",
    "Dick Grune, Kees van Reeuwijk, Henri E. Bal, Ceriel J.H. Jacobs, Koen Langendoen, \"Modern Compiler Design\", Springer, 2012.\n",
    "\n",
    "Frost, R., Hafiz, R. and Callaghan, P. \"Modular and Efficient Top-Down Parsing for Ambiguous Left-Recursive Grammars .\", 2007\n",
    "\n",
    "Aho, Alfred V., Ullman, Jeffrey D. \"The Theory of Parsing, Translation, and Compiling\" Englewood Cliffs, NJ: Prentice-Hall. 1972\n",
    "\n",
    "Rosenkrantz, D. J., Stearns, R. E., \"Properties of Deterministic Top Down Grammars\", 1970"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
